import argparse
import os
import time
from faster_whisper import WhisperModel, BatchedInferencePipeline
from tqdm import tqdm

# Model VinAI
DEFAULT_MODEL = "suzii/vi-whisper-large-v3-turbo-v1-ct2"

# Máº¹o: CÃ¢u prompt giÃºp model Ä‘á»‹nh hÃ¬nh cÃ¡ch viáº¿t hoa vÃ  ngáº¯t cÃ¢u
# Chá»©a: TÃªn riÃªng, Äá»‹a danh, Dáº¥u cháº¥m, Dáº¥u pháº©y, Dáº¥u há»i.
VIETNAMESE_PROMPT = "Xin chÃ o cÃ¡c báº¡n. ÄÃ¢y lÃ  báº£n ghi chÃ©p chÃ­nh xÃ¡c, cÃ³ Ä‘áº§y Ä‘á»§ dáº¥u cháº¥m, dáº¥u pháº©y. TÃªn riÃªng nhÆ° HÃ  Ná»™i, Há»“ ChÃ­ Minh, VinAI Ä‘á»u Ä‘Æ°á»£c viáº¿t hoa chuáº©n xÃ¡c."

def format_timestamp(seconds):
    whole_seconds = int(seconds)
    milliseconds = int((seconds - whole_seconds) * 1000)
    hours = whole_seconds // 3600
    minutes = (whole_seconds % 3600) // 60
    seconds = whole_seconds % 60
    return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"

def main():
    parser = argparse.ArgumentParser(description="Vietnamese Speech-to-Text (Fix Punctuation & Capitalization)")
    parser.add_argument("input_file", help="Path to audio file")
    parser.add_argument("-o", "--output", help="Output file path")
    parser.add_argument("--batch_size", type=int, default=8, help="Lower batch size helps accuracy slightly")
    parser.add_argument("--device", default="cuda", help="cuda or cpu")
    parser.add_argument("--beam_size", type=int, default=5, help="Higher beam size (5) improves punctuation accuracy")
    
    args = parser.parse_args()

    if not os.path.exists(args.input_file):
        print(f"âŒ File not found: {args.input_file}")
        return

    if not args.output:
        args.output = os.path.splitext(args.input_file)[0] + ".txt"

    print(f"--- ðŸš€ Loading Model: {DEFAULT_MODEL} ---")
    
    try:
        model = WhisperModel(DEFAULT_MODEL, device=args.device, compute_type="float16")
        batched_model = BatchedInferencePipeline(model=model)
    except Exception as e:
        print(f"âŒ Error: {e}")
        return

    print(f"--- âš¡ Processing with Prompt Context ---")
    start_time = time.time()
    
    # THAY Äá»”I QUAN TRá»ŒNG:
    # 1. beam_size=5: TÃ¬m kiáº¿m ká»¹ hÆ¡n Ä‘á»ƒ Ä‘áº·t dáº¥u cÃ¢u Ä‘Ãºng.
    # 2. initial_prompt: Ã‰p model tuÃ¢n thá»§ quy táº¯c viáº¿t hoa.
    segments, info = batched_model.transcribe(
        args.input_file, 
        batch_size=args.batch_size, 
        beam_size=args.beam_size, 
        initial_prompt=VIETNAMESE_PROMPT
    )

    print(f"    Language: {info.language}")

    with open(args.output, "w", encoding="utf-8") as f:
        with tqdm(total=round(info.duration), unit="sec") as pbar:
            last_pos = 0
            for segment in segments:
                text = segment.text.strip()
                
                # Logic nhá»: Náº¿u cÃ¢u trÆ°á»›c káº¿t thÃºc báº±ng dáº¥u cÃ¢u, cÃ¢u sau nÃªn viáº¿t hoa (Whisper thÆ°á»ng tá»± lÃ m, nhÆ°ng check thÃªm cho cháº¯c)
                if text and not text[0].isupper():
                     text = text[0].upper() + text[1:]

                # line = f"[{format_timestamp(segment.start)} -> {format_timestamp(segment.end)}] {text}\n"
                line = f"{text}\n"
                f.write(line)
                
                pbar.update(round(segment.end - last_pos))
                last_pos = segment.end

    print(f"\n--- âœ… Done! Saved to: {args.output} ---")

if __name__ == "__main__":
    main()
