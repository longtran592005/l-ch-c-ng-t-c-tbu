"""
TBU AI Service - FastAPI
Integrates Whisper (GPU) + Qwen 2.5 (CPU) for meeting analysis
Singleton pattern for Qwen model to prevent OOM
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List
import shutil
import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import logging

from vinai import transcribe_audio, load_model as load_whisper_model
from config import (
    WHISPER_MODEL, DEVICE, WHISPER_SIZE,
    QWEN_MODEL, QWEN_MAX_NEW_TOKENS, QWEN_TEMPERATURE, QWEN_TOP_P
)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="TBU AI Service", version="1.0.0")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global state
UPLOAD_DIR = "temp_uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# Qwen Model (Singleton - loaded once)
_qwen_model = None
_qwen_tokenizer = None

def load_qwen_model():
    """
    Load Qwen 2.5 model once on startup (singleton pattern).
    Runs on CPU to save GPU VRAM for Whisper.
    """
    global _qwen_model, _qwen_tokenizer
    
    if _qwen_model is not None:
        logger.info("‚úÖ Qwen model already loaded")
        return _qwen_model, _qwen_tokenizer
    
    try:
        logger.info(f"üöÄ Loading Qwen model on CPU: {QWEN_MODEL}")
        logger.info("‚ö†Ô∏è  This will take 1-2 minutes on first run...")
        
        # Load on CPU to save GPU for Whisper
        _qwen_tokenizer = AutoTokenizer.from_pretrained(
            QWEN_MODEL,
            trust_remote_code=True
        )
        
        _qwen_model = AutoModelForCausalLM.from_pretrained(
            QWEN_MODEL,
            torch_dtype=torch.float32,  # CPU runs better with float32
            device_map="cpu",
            trust_remote_code=True,
            low_cpu_mem_usage=True,  # Critical for 13GB RAM
            offload_folder="offload",  # Offload to disk if needed
        )
        
        logger.info(f"‚úÖ Qwen model loaded on CPU")
        logger.info(f"   Model size: {_qwen_model.num_parameters / 1e9:.1f}B parameters")
        
        return _qwen_model, _qwen_tokenizer
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load Qwen model: {e}")
        raise HTTPException(status_code=503, detail="Qwen model not available")

def clean_transcript(text: str) -> str:
    """
    Clean transcript by removing filler words and normalizing.
    Optimized for Vietnamese meeting transcripts.
    """
    if not text:
        return ""
    
    # Vietnamese filler words commonly found in meetings
    filler_words = [
        "√†", "·ªù", "·ª´", "·ª´m", "√† m√†", "th√¨ m√†",
        "nh∆∞ v·∫≠y", "th√¨ l√†", "r·ªìi th√¨", "ƒë√∫ng kh√¥ng",
        "v√≠ d·ª• nh∆∞", "ch·∫≥ng h·∫°n", "t·ª©c l√†", "√Ω l√†"
    ]
    
    # Remove extra whitespace
    lines = text.strip().split('\n')
    cleaned_lines = []
    
    for line in lines:
        cleaned = line.strip()
        
        # Remove multiple spaces
        while '  ' in cleaned:
            cleaned = cleaned.replace('  ', ' ')
        
        # Remove filler words at the beginning of sentences
        for filler in filler_words:
            if cleaned.startswith(filler + ' '):
                cleaned = cleaned[len(filler):].strip()
            elif cleaned.startswith(filler + ','):
                cleaned = cleaned[len(filler):].strip()
        
        if cleaned:
            cleaned_lines.append(cleaned)
    
    return '\n'.join(cleaned_lines)

def generate_with_qwen(prompt: str, max_tokens: int = QWEN_MAX_NEW_TOKENS) -> str:
    """
    Generate text using Qwen 2.5 model (CPU).
    """
    global _qwen_model, _qwen_tokenizer
    
    if _qwen_model is None:
        load_qwen_model()
    
    # Apply chat template
    messages = [
        {"role": "system", "content": prompt}
    ]
    
    text = _qwen_tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # Tokenize
    inputs = _qwen_tokenizer(text, return_tensors="pt").to("cpu")
    
    # Generate with CPU-optimized settings
    with torch.no_grad():
        outputs = _qwen_model.generate(
            inputs,
            max_new_tokens=max_tokens,
            temperature=QWEN_TEMPERATURE,
            top_p=QWEN_TOP_P,
            do_sample=True,
            pad_token_id=_qwen_tokenizer.eos_token_id,
            eos_token_id=_qwen_tokenizer.eos_token_id,
            use_cache=True,  # Enable KV cache
        )
    
    # Decode
    generated_text = _qwen_tokenizer.decode(
        outputs[0],
        skip_special_tokens=True
    )
    
    # Extract only the new part
    new_text = generated_text[len(text):].strip()
    
    return new_text

# ==================== Pydantic Models ====================

class AnalyzeRequest(BaseModel):
    """Request model for AI analysis"""
    transcript: str
    generate_summary: bool = True
    generate_minutes: bool = False
    extract_action_items: bool = False
    deep_analysis: bool = False
    max_tokens: Optional[int] = None

class ActionItem(BaseModel):
    """Action item extracted from meeting"""
    task: str
    assignee: Optional[str] = None
    deadline: Optional[str] = None
    priority: str = "medium"
    notes: Optional[str] = None

class AnalyzeResponse(BaseModel):
    """Response model for AI analysis"""
    original_transcript: str
    cleaned_transcript: str
    summary: Optional[str] = None
    minutes: Optional[str] = None
    action_items: Optional[List[ActionItem]] = None
    analysis: Optional[str] = None
    processing_time: float

# ==================== Endpoints ====================

@app.on_event("startup")
async def startup_event():
    """Load models on startup"""
    logger.info("=" * 60)
    logger.info("TBU AI Service Starting...")
    logger.info("=" * 60)
    
    # Load Whisper model
    try:
        load_whisper_model()
        logger.info(f"‚úÖ Whisper model loaded: {WHISPER_MODEL}")
    except Exception as e:
        logger.error(f"‚ùå Failed to load Whisper: {e}")
    
    # Pre-load Qwen model (singleton)
    try:
        load_qwen_model()
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Qwen model load failed, will lazy-load: {e}")

@app.get("/")
async def root():
    """Health check"""
    return {
        "service": "tbu-ai-service",
        "status": "running",
        "models": {
            "whisper": "loaded" if torch.cuda.is_available() or DEVICE == "cpu" else "loading",
            "qwen": "loaded" if _qwen_model is not None else "not_loaded",
            "qwen_model": QWEN_MODEL,
            "whisper_device": DEVICE
        }
    }

@app.post("/ai/analyze")
async def analyze_meeting(request: AnalyzeRequest):
    """
    Complete AI analysis pipeline:
    1. Clean transcript
    2. Generate summary (if requested)
    3. Generate minutes (if requested)
    4. Extract action items (if requested)
    5. Deep analysis (if requested)
    
    All runs on CPU with Qwen 2.5 for stability.
    """
    import time
    start_time = time.time()
    
    logger.info(f"üìä Starting AI analysis ({len(request.transcript)} chars)")
    
    # Step 1: Clean transcript
    cleaned_text = clean_transcript(request.transcript)
    logger.info(f"‚ú® Text cleaned ({len(request.transcript)} -> {len(cleaned_text)} chars)")
    
    response = AnalyzeResponse(
        original_transcript=request.transcript,
        cleaned_transcript=cleaned_text
    )
    
    # Generate outputs in parallel (same model, different prompts)
    import asyncio
    
    async def generate_summary():
        prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n nghi·ªáp chuy√™n t√≥m t·∫Øt n·ªôi dung cu·ªôc h·ªçp.
Nhi·ªám v·ª•: T√≥m t·∫Øt ng·∫Øn g·ªçn, s√∫c t√≠ch c√°c ƒëi·ªÉm ch√≠nh c·ªßa cu·ªôc h·ªçp.
Y√™u c·∫ßu:
- T√≥m t·∫Øt trong 3-5 ƒëo·∫°n
- N√™u r√µ m·ª•c ti√™u ch√≠nh
- Li·ªát k√™ c√°c quy·∫øt ƒë·ªãnh quan tr·ªçng
- Gi·ªçng vƒÉn chuy√™n nghi·ªáp, kh√°ch quan

N·ªôi dung cu·ªôc h·ªçp:
{content}

T√≥m t·∫Øt:""".format(content=cleaned_text[:8000])  # Limit to 8k chars for stability
        
        try:
            summary_text = generate_with_qwen(prompt, max_tokens=1024)
            logger.info(f"‚úÖ Summary generated ({len(summary_text)} chars)")
            response.summary = summary_text
        except Exception as e:
            logger.error(f"‚ùå Summary failed: {e}")
            response.summary = "L·ªói t·∫°o t√≥m t·∫Øt"
    
    async def generate_minutes():
        prompt = """B·∫°n l√† th∆∞ k√Ω chuy√™n nghi·ªáp c√≥ kinh nghi·ªám so·∫°n th·∫£o bi√™n b·∫£n cu·ªôc h·ªçp.
Nhi·ªám v·ª•: T·∫°o bi√™n b·∫£n cu·ªôc h·ªçp ƒë·∫ßy ƒë·ªß, c√≥ c·∫•u tr√∫c r√µ r√†ng.
C·∫•u tr√∫c bi√™n b·∫£n:
1. TH√îNG TIN CHUNG
   - T√™n cu·ªôc h·ªçp
   - Th·ªùi gian, ƒë·ªãa ƒëi·ªÉm
   - Th√†nh ph·∫ßn tham d·ª±

2. N·ªòI DUNG CU·ªòC H·ªåP
   - C√°c m·ª•c th·∫£o lu·∫≠n ch√≠nh
   - Quan ƒëi·ªÉm v√† tranh lu·∫≠n

3. QUY·∫æT ƒê·ªäNH
   - C√°c quy·∫øt ƒë·ªãnh ƒë√£ th√¥ng qua
   - K·∫øt qu·∫£ bi·ªÉu quy·∫øt (n·∫øu c√≥)

4. H√ÄNH ƒê·ªòNG C·∫¶N L√ÄM
   - C√°c c√¥ng vi·ªác c·∫ßn th·ª±c hi·ªán
   - Ng∆∞·ªùi ph·ª• tr√°ch
   - Th·ªùi h·∫°n ho√†n th√†nh

Y√™u c·∫ßu: Ng√¥n ng·ªØ trang tr·ªçng, ch√≠nh x√°c, ƒë·∫ßy ƒë·ªß th√¥ng tin.

N·ªôi dung cu·ªôc h·ªçp:
{content}

Bi√™n b·∫£n:""".format(content=cleaned_text[:12000])  # Limit to 12k chars
        
        try:
            minutes_text = generate_with_qwen(prompt, max_tokens=3072)
            logger.info(f"‚úÖ Minutes generated ({len(minutes_text)} chars)")
            response.minutes = minutes_text
        except Exception as e:
            logger.error(f"‚ùå Minutes failed: {e}")
            response.minutes = "L·ªói t·∫°o bi√™n b·∫£n"
    
    async def extract_action_items():
        prompt = """B·∫°n l√† tr·ª£ l√Ω qu·∫£n l√Ω d·ª± √°n chuy√™n nghi·ªáp.
Nhi·ªám v·ª•: Tr√≠ch xu·∫•t c√°c h√†nh ƒë·ªông c·∫ßn l√†m t·ª´ n·ªôi dung cu·ªôc h·ªçp.
Y√™u c·∫ßu:
- Nh·∫≠n d·∫°ng r√µ: Vi·ªác c·∫ßn l√†m, ng∆∞·ªùi ph·ª• tr√°ch, deadline
- Format JSON:
  {{
    "action_items": [
      {{
        "task": "M√¥ t·∫£ c√¥ng vi·ªác",
        "assignee": "Ng∆∞·ªùi ph·ª• tr√°ch",
        "deadline": "Ng√†y gi·ªù ho·∫∑c null",
        "priority": "high/medium/low",
        "notes": "Ghi ch√∫ b·ªï sung"
      }}
    ]
  }}
- Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng c√≥ vƒÉn b·∫£n kh√°c

N·ªôi dung cu·ªôc h·ªçp:
{content}

Action items (JSON format):""".format(content=cleaned_text[:8000])
        
        try:
            items_text = generate_with_qwen(prompt, max_tokens=2048)
            logger.info(f"‚úÖ Action items extracted")
            
            # Parse JSON from response
            import json
            try:
                json_start = items_text.find('{')
                json_end = items_text.rfind('}') + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = items_text[json_start:json_end]
                    data = json.loads(json_str)
                    response.action_items = data.get('action_items', [])
                else:
                    response.action_items = []
            except json.JSONDecodeError as je:
                logger.error(f"‚ùå JSON parse failed: {je}")
                response.action_items = []
        except Exception as e:
            logger.error(f"‚ùå Action items failed: {e}")
            response.action_items = []
    
    async def perform_analysis():
        prompt = """B·∫°n l√† nh√† ph√¢n t√≠ch kinh doanh chuy√™n nghi·ªáp.
Nhi·ªám v·ª•: Ph√¢n t√≠ch s√¢u n·ªôi dung cu·ªôc h·ªçp.
Ph√¢n t√≠ch c√°c kh√≠a c·∫°nh:
1. M·ª§C TI√äU V√Ä PH·∫†M VI
   - M·ª•c ti√™u cu·ªôc h·ªçp c√≥ r√µ r√†ng kh√¥ng?
   - C√°c v·∫•n ƒë·ªÅ ƒë√£ ƒë∆∞·ª£c th·∫£o lu·∫≠n ƒë·ªß ch∆∞a?

2. THAM GIA
   - Th√†nh ph·∫ßn tham d·ª± c√≥ ph√π h·ª£p kh√¥ng?
   - Ai ƒë√≥ng g√≥p t√≠ch c·ª±c nh·∫•t?

3. V·∫§N ƒê·ªÄ V√Ä TH√ÅCH TH·ª®C
   - C√°c tr·ªü ng·∫°i ch√≠nh
   - R·ªßi ro ti·ªÅm ·∫©n

4. QUY·∫æT ƒê·ªäNH V√Ä H∆Ø·ªöNG GI·∫¢I QUY·∫æT
   - C√°c quy·∫øt ƒë·ªãnh ƒë√£ ƒë·∫°t
   - Ph∆∞∆°ng √°n gi·∫£i quy·∫øt

5. ƒê·ªÄ XU·∫§T
   - C·∫£i ti·∫øn cho cu·ªôc h·ªçp ti·∫øp theo
   - H√†nh ƒë·ªông theo d√µi

Y√™u c·∫ßu: Ph√¢n t√≠ch chuy√™n nghi·ªáp, chi ti·∫øt, c√≥ c∆° s·ªü.

N·ªôi dung cu·ªôc h·ªçp:
{content}

Ph√¢n t√≠ch chi ti·∫øt:""".format(content=cleaned_text[:12000])
        
        try:
            analysis_text = generate_with_qwen(prompt, max_tokens=3072)
            logger.info(f"‚úÖ Deep analysis completed ({len(analysis_text)} chars)")
            response.analysis = analysis_text
        except Exception as e:
            logger.error(f"‚ùå Deep analysis failed: {e}")
            response.analysis = "L·ªói ph√¢n t√≠ch s√¢u"
    
    # Run requested tasks
    tasks = []
    if request.generate_summary:
        tasks.append(generate_summary())
    if request.generate_minutes:
        tasks.append(generate_minutes())
    if request.extract_action_items:
        tasks.append(extract_action_items())
    if request.deep_analysis:
        tasks.append(perform_analysis())
    
    # Execute tasks sequentially (not parallel due to CPU)
    for task in tasks:
        await task
    
    processing_time = time.time() - start_time
    
    logger.info(f"‚úÖ AI analysis complete in {processing_time:.1f}s")
    response.processing_time = processing_time
    
    return response

@app.post("/transcribe")
async def transcribe_endpoint(file: UploadFile = File(...)):
    """
    Legacy endpoint for backward compatibility.
    Just transcribes audio to text without AI analysis.
    """
    try:
        file_path = os.path.join(UPLOAD_DIR, file.filename)
        
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        logger.info(f"üìÇ Received file: {file.filename} ({os.path.getsize(file_path)} bytes)")
        
        # Transcribe with Whisper
        text = transcribe_audio(file_path)
        
        # Clean up
        if os.path.exists(file_path):
            os.remove(file_path)
        
        logger.info(f"‚úÖ Transcription complete")
        
        return {"text": text}
    
    except Exception as e:
        logger.error(f"‚ùå Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
