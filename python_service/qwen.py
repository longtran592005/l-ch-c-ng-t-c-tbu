"""
Qwen 2.5 Instruct Model Integration for Meeting Analysis
Supports: Summary, Meeting Minutes, Action Items Extraction, Deep Analysis
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from typing import Dict, List, Optional
import json
import logging

logger = logging.getLogger(__name__)

# Model configuration
DEFAULT_MODEL = "Qwen/Qwen2.5-7B-Instruct"

# System prompts for different tasks
SYSTEM_PROMPTS = {
    "summary": """Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn nghiá»‡p chuyÃªn tÃ³m táº¯t ná»™i dung cuá»™c há»p.
Nhiá»‡m vá»¥: TÃ³m táº¯t ngáº¯n gá»n, sÃºc tÃ­ch cÃ¡c Ä‘iá»ƒm chÃ­nh cá»§a cuá»™c há»p.
YÃªu cáº§u:
- TÃ³m táº¯t trong 3-5 Ä‘oáº¡n
- NÃªu rÃµ má»¥c tiÃªu chÃ­nh
- Liá»‡t kÃª cÃ¡c quyáº¿t Ä‘á»‹nh quan trá»ng
- Giá»ng vÄƒn chuyÃªn nghiá»‡p, khÃ¡ch quan""",

    "minutes": """Báº¡n lÃ  thÆ° kÃ½ chuyÃªn nghiá»‡p cÃ³ kinh nghiá»‡m soáº¡n tháº£o biÃªn báº£n cuá»™c há»p.
Nhiá»‡m vá»¥: Táº¡o biÃªn báº£n cuá»™c há»p Ä‘áº§y Ä‘á»§, cÃ³ cáº¥u trÃºc rÃµ rÃ ng.
Cáº¥u trÃºc biÃªn báº£n:
1. THÃ”NG TIN CHUNG
   - TÃªn cuá»™c há»p
   - Thá»i gian, Ä‘á»‹a Ä‘iá»ƒm
   - ThÃ nh pháº§n tham dá»±
2. Ná»˜I DUNG CUá»˜C Há»ŒP
   - CÃ¡c má»¥c tháº£o luáº­n chÃ­nh
   - Quan Ä‘iá»ƒm vÃ  tranh luáº­n
3. QUYáº¾T Äá»ŠNH
   - CÃ¡c quyáº¿t Ä‘á»‹nh Ä‘Ã£ thÃ´ng qua
   - Káº¿t quáº£ biá»ƒu quyáº¿t (náº¿u cÃ³)
4. HÃ€NH Äá»˜NG Cáº¦N LÃ€M
   - CÃ¡c cÃ´ng viá»‡c cáº§n thá»±c hiá»‡n
   - NgÆ°á»i phá»¥ trÃ¡ch
   - Thá»i háº¡n hoÃ n thÃ nh
5. Lá»ŠCH TRÃŒNH TIáº¾P THEO
   - Cuá»™c há»p tiáº¿p theo (náº¿u cÃ³)
YÃªu cáº§u: NgÃ´n ngá»¯ trang trá»ng, chÃ­nh xÃ¡c, Ä‘áº§y Ä‘á»§ thÃ´ng tin.""",

    "action_items": """Báº¡n lÃ  trá»£ lÃ½ quáº£n lÃ½ dá»± Ã¡n chuyÃªn nghiá»‡p.
Nhiá»‡m vá»¥: TrÃ­ch xuáº¥t cÃ¡c hÃ nh Ä‘á»™ng cáº§n lÃ m tá»« ná»™i dung cuá»™c há»p.
YÃªu cáº§u:
- Nháº­n dáº¡ng rÃµ: Viá»‡c cáº§n lÃ m, ngÆ°á»i phá»¥ trÃ¡ch, deadline
- Format JSON:
  {
    "action_items": [
      {
        "task": "MÃ´ táº£ cÃ´ng viá»‡c",
        "assignee": "NgÆ°á»i phá»¥ trÃ¡ch",
        "deadline": "NgÃ y giá» hoáº·c null",
        "priority": "high/medium/low",
        "notes": "Ghi chÃº bá»• sung"
      }
    ]
  }
- Chá»‰ tráº£ vá» JSON, khÃ´ng cÃ³ vÄƒn báº£n khÃ¡c""",

    "deep_analysis": """Báº¡n lÃ  nhÃ  phÃ¢n tÃ­ch kinh doanh chuyÃªn nghiá»‡p.
Nhiá»‡m vá»¥: PhÃ¢n tÃ­ch sÃ¢u ná»™i dung cuá»™c há»p.
PhÃ¢n tÃ­ch cÃ¡c khÃ­a cáº¡nh:
1. Má»¤C TIÃŠU VÃ€ PHáº M VI
   - Má»¥c tiÃªu cuá»™c há»p cÃ³ rÃµ rÃ ng khÃ´ng?
   - CÃ¡c váº¥n Ä‘á» Ä‘Ã£ Ä‘Æ°á»£c tháº£o luáº­n Ä‘á»§ chÆ°a?

2. THAM GIA
   - ThÃ nh pháº§n tham dá»± cÃ³ phÃ¹ há»£p khÃ´ng?
   - Ai Ä‘Ã³ng gÃ³p tÃ­ch cá»±c nháº¥t?

3. Váº¤N Äá»€ VÃ€ THÃCH THá»¨C
   - CÃ¡c trá»Ÿ ngáº¡i chÃ­nh
   - Rá»§i ro tiá»m áº©n

4. QUYáº¾T Äá»ŠNH VÃ€ HÆ¯á»šNG GIáº¢I QUYáº¾T
   - CÃ¡c quyáº¿t Ä‘á»‹nh Ä‘Ã£ Ä‘áº¡t
   - PhÆ°Æ¡ng Ã¡n giáº£i quyáº¿t

5. Äá»€ XUáº¤T
   - Cáº£i tiáº¿n cho cuá»™c há»p tiáº¿p theo
   - HÃ nh Ä‘á»™ng theo dÃµi

YÃªu cáº§u: PhÃ¢n tÃ­ch chuyÃªn nghiá»‡p, chi tiáº¿t, cÃ³ cÆ¡ sá»Ÿ.""",

    "meeting_insights": """Báº¡n lÃ  nhÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u cuá»™c há»p chuyÃªn nghiá»‡p.
Nhiá»‡m vá»¥: PhÃ¢n tÃ­ch vÃ  tá»•ng há»£p thÃ´ng tin chi tiáº¿t vá» cuá»™c há»p.
YÃªu cáº§u:
- PhÃ¢n tÃ­ch tÃ¢m tráº¡ng vÃ  thÃ¡i Ä‘á»™ tham gia
- XÃ¡c Ä‘á»‹nh cÃ¡c váº¥n Ä‘á» tranh luáº­n
- TÃ¬m kiáº¿m cÃ¡c cÆ¡ há»™i vÃ  thÃ¡ch thá»©c
- ÄÃ¡nh giÃ¡ hiá»‡u quáº£ cuá»™c há»p
- Format: Markdown vá»›i cÃ¡c tiÃªu Ä‘á» rÃµ rÃ ng"""
}

class QwenModel:
    """Qwen 2.5 Model Manager with quantization support"""

    def __init__(self, model_name: str = DEFAULT_MODEL, device: Optional[str] = None):
        self.model_name = model_name
        self.device = device or self._detect_device()
        self.model = None
        self.tokenizer = None
        self._load_model()

    def _detect_device(self) -> str:
        """Detect available device (CUDA > CPU)"""
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            device_name = torch.cuda.get_device_name(0)
            vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            logger.info(f"ðŸŽ® CUDA Available: {device_count} device(s)")
            logger.info(f"ðŸŽ® GPU: {device_name}")
            logger.info(f"ðŸŽ® VRAM: {vram:.1f}GB")
            
            return "cuda"
        else:
            logger.info("âš ï¸ CUDA NOT Available - Using CPU")
            return "cpu"

    def _load_model(self):
        """Load Qwen model with appropriate quantization"""
        if self.model is not None:
            return self.model

        try:
            logger.info(f"ðŸš€ Loading Qwen Model: {self.model_name}")

            # Configure quantization for CUDA
            quantization_config = None
            if self.device == "cuda":
                # 4-bit quantization for memory efficiency
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                logger.info("ðŸ“¦ Using 4-bit quantization (NF4)")

            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )

            # Load model
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=quantization_config,
                device_map="auto" if self.device == "cuda" else None,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                trust_remote_code=True
            )

            logger.info(f"âœ… Qwen Model loaded successfully on {self.device.upper()}")

        except Exception as e:
            logger.error(f"âŒ Failed to load Qwen model: {e}")
            raise

    def _generate_response(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_new_tokens: int = 2048,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> str:
        """Generate response from Qwen model"""
        try:
            # Prepare messages
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            # Apply chat template
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            # Tokenize
            inputs = self.tokenizer(
                [text],
                return_tensors="pt",
                padding=True
            ).to(self.model.device)

            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )

            # Decode
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )

            return response.strip()

        except Exception as e:
            logger.error(f"Error generating response: {e}")
            raise

    def generate_summary(self, meeting_content: str) -> str:
        """Generate meeting summary"""
        system_prompt = SYSTEM_PROMPTS["summary"]
        prompt = f"""TÃ³m táº¯t cuá»™c há»p sau:

{meeting_content}

TÃ³m táº¯t:"""

        return self._generate_response(
            prompt,
            system_prompt=system_prompt,
            max_new_tokens=1024,
            temperature=0.6
        )

    def generate_minutes(self, meeting_content: str, additional_info: Optional[Dict] = None) -> str:
        """Generate meeting minutes with structure"""
        system_prompt = SYSTEM_PROMPTS["minutes"]

        # Add additional info if provided
        prompt = f"""Táº¡o biÃªn báº£n cho cuá»™c há»p sau:"""
        
        if additional_info:
            prompt += f"""
THÃ”NG TIN CÆ  Báº¢N:
- TÃªn cuá»™c há»p: {additional_info.get('title', 'KhÃ´ng rÃµ')}
- NgÃ y giá»: {additional_info.get('meeting_date', 'KhÃ´ng rÃµ')}
- Äá»‹a Ä‘iá»ƒm: {additional_info.get('location', 'KhÃ´ng rÃµ')}
- NgÆ°á»i Ä‘iá»u hÃ nh: {additional_info.get('leader', 'KhÃ´ng rÃµ')}
"""

        prompt += f"""

Ná»˜I DUNG CUá»˜C Há»ŒP:
{meeting_content}

BiÃªn báº£n:"""

        return self._generate_response(
            prompt,
            system_prompt=system_prompt,
            max_new_tokens=3072,
            temperature=0.7
        )

    def extract_action_items(self, meeting_content: str) -> Dict:
        """Extract action items from meeting"""
        system_prompt = SYSTEM_PROMPTS["action_items"]
        prompt = f"""TrÃ­ch xuáº¥t cÃ¡c hÃ nh Ä‘á»™ng cáº§n lÃ m tá»« ná»™i dung cuá»™c há»p:

{meeting_content}

Action items (JSON format):"""

        response = self._generate_response(
            prompt,
            system_prompt=system_prompt,
            max_new_tokens=1536,
            temperature=0.5
        )

        try:
            # Extract JSON from response
            json_start = response.find('{')
            json_end = response.rfind('}') + 1

            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                return json.loads(json_str)
            else:
                logger.warning("No valid JSON found in response")
                return {"action_items": []}

        except json.JSONDecodeError as e:
            logger.error(f"Error parsing JSON: {e}")
            return {"action_items": []}

    def deep_analysis(self, meeting_content: str) -> str:
        """Perform deep analysis of meeting content"""
        system_prompt = SYSTEM_PROMPTS["deep_analysis"]
        prompt = f"""PhÃ¢n tÃ­ch sÃ¢u ná»™i dung cuá»™c há»p sau:

{meeting_content}

PhÃ¢n tÃ­ch chi tiáº¿t:"""

        return self._generate_response(
            prompt,
            system_prompt=system_prompt,
            max_new_tokens=3072,
            temperature=0.7
        )

    def meeting_insights(self, meeting_content: str) -> str:
        """Generate meeting insights and patterns"""
        system_prompt = SYSTEM_PROMPTS["meeting_insights"]
        prompt = f"""PhÃ¢n tÃ­ch vÃ  tá»•ng há»£p thÃ´ng tin chi tiáº¿t vá» cuá»™c há»p:

{meeting_content}

ThÃ´ng tin phÃ¢n tÃ­ch:"""

        return self._generate_response(
            prompt,
            system_prompt=system_prompt,
            max_new_tokens=2048,
            temperature=0.7
        )


# Global model instance
_qwen_model: Optional[QwenModel] = None

def load_qwen_model(model_name: str = DEFAULT_MODEL, device: Optional[str] = None) -> QwenModel:
    """Load or return cached Qwen model"""
    global _qwen_model
    if _qwen_model is None:
        _qwen_model = QwenModel(model_name, device)
    return _qwen_model

def get_qwen_model() -> Optional[QwenModel]:
    """Get current Qwen model instance"""
    return _qwen_model
