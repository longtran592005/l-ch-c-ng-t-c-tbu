# Python Speech-to-Text Service Configuration

# Server Configuration
PORT=8001
HOST=0.0.0.0

# Model Configuration
# Model: vinai/PhoWhisper-small (hoáº·c: tiny, base, small, medium, large)
WHISPER_MODEL=vinai/PhoWhisper-small
WHISPER_SIZE=small

# Device Configuration
# Options: cuda, cpu, mps (macOS)
DEVICE=cpu

# Computation Type
# Options: float16 (GPU), int8 (CPU)
COMPUTE_TYPE=int8

# Transcription Configuration
DEFAULT_LANGUAGE=vi
DEFAULT_TASK=transcribe

# Format Settings
DEFAULT_FORMAT_OUTPUT=true
AUTO_LINE_BREAKS=true
MAX_SENTENCE_LENGTH=300

# File Configuration
MAX_FILE_SIZE=524288000  # 500MB in bytes

# Timeout Settings (in seconds)
UPLOAD_TIMEOUT=300
TRANSCRIPTION_TIMEOUT=600

# Logging Configuration
LOG_LEVEL=INFO

# CORS Configuration
CORS_ORIGINS=*
CORS_ALLOW_CREDENTIALS=true

# Hugging Face Configuration (optional)
# HF_TOKEN=your_huggingface_token_here
# HF_CACHE_DIR=./models

# Qwen 2.5 Configuration
# Enable/disable Qwen model (true/false)
QWEN_ENABLED=true

# Qwen model name (Qwen/Qwen2.5-7B-Instruct recommended)
# Other options: Qwen/Qwen2.5-14B-Instruct, Qwen/Qwen2.5-72B-Instruct
QWEN_MODEL=Qwen/Qwen2.5-7B-Instruct

# Max new tokens for generation (default: 3072)
QWEN_MAX_NEW_TOKENS=3072

# Temperature for generation (default: 0.7)
# Lower = more focused, Higher = more creative
QWEN_TEMPERATURE=0.7

# Top P for nucleus sampling (default: 0.9)
QWEN_TOP_P=0.9

# Use 4-bit quantization for memory efficiency (recommended for mid-range GPUs)
# Set to false if using high-end GPU (24GB+ VRAM)
QWEN_USE_QUANTIZATION=true
