"""
Ollama qwen2.5:7b Wrapper for Response Generation
S·ª≠ d·ª•ng Ollama local ƒë·ªÉ generate response t·ª´ RAG context

@author TBU AI Team
"""
import httpx
import json
from typing import List, Dict, Optional
import logging
import asyncio
import sys
import os

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from rag_config import (
    OLLAMA_BASE_URL, 
    OLLAMA_MODEL, 
    LLM_TEMPERATURE, 
    LLM_MAX_TOKENS,
    LLM_TIMEOUT,
    LLM_KEEP_ALIVE
)

logger = logging.getLogger(__name__)


# System prompt cho chatbot TBU
SYSTEM_PROMPT = """B·∫°n l√† Tr·ª£ l√Ω ·∫£o TBU - chatbot h·ªó tr·ª£ tra c·ª©u th√¥ng tin cho Tr∆∞·ªùng ƒê·∫°i h·ªçc Th√°i B√¨nh.

NHI·ªÜM V·ª§ C·ª¶A B·∫†N:
1. Tr·∫£ l·ªùi c√¢u h·ªèi D·ª∞A TR√äN th√¥ng tin trong CONTEXT ƒë∆∞·ª£c cung c·∫•p
2. N·∫øu th√¥ng tin KH√îNG C√ì trong CONTEXT, h√£y n√≥i r√µ l√† b·∫°n kh√¥ng c√≥ th√¥ng tin ƒë√≥
3. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c, th√¢n thi·ªán v√† chuy√™n nghi·ªáp
4. S·ª≠ d·ª•ng format markdown khi c·∫ßn (bold, bullet points, numbered list)
5. LU√îN tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI V·ªÄ L·ªäCH C√îNG T√ÅC:
- Khi c√≥ l·ªãch: Li·ªát k√™ ƒê·∫¶Y ƒê·ª¶ th√¥ng tin theo format:
  ‚Ä¢ **Th·ªùi gian**: [gi·ªù b·∫Øt ƒë·∫ßu - gi·ªù k·∫øt th√∫c]
  ‚Ä¢ **N·ªôi dung**: [m√¥ t·∫£ ho·∫°t ƒë·ªông]
  ‚Ä¢ **ƒê·ªãa ƒëi·ªÉm**: [n∆°i di·ªÖn ra]
  ‚Ä¢ **Ch·ªß tr√¨**: [ng∆∞·ªùi ch·ªß tr√¨]
  ‚Ä¢ **Th√†nh ph·∫ßn**: [ai tham d·ª±]
- Khi KH√îNG c√≥ l·ªãch: Tr·∫£ l·ªùi r√µ r√†ng "Kh√¥ng c√≥ l·ªãch c√¥ng t√°c v√†o [th·ªùi gian]"
- N·∫øu c√≥ nhi·ªÅu l·ªãch, li·ªát k√™ theo th·ª© t·ª± th·ªùi gian

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI KH√ÅC:
- V·ªÅ TIN T·ª®C/TH√îNG B√ÅO: T√≥m t·∫Øt n·ªôi dung ch√≠nh, n√™u ng√†y ƒëƒÉng
- V·ªÅ TH√îNG TIN TR∆Ø·ªúNG: Cung c·∫•p th√¥ng tin ch√≠nh x√°c t·ª´ context

L∆ØU √ù QUAN TR·ªåNG:
- KH√îNG b·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng c√≥ trong context
- N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, n√≥i "Theo th√¥ng tin t√¥i c√≥..."
- N·∫øu context r·ªóng ho·∫∑c kh√¥ng li√™n quan, th√¥ng b√°o kh√¥ng t√¨m th·∫•y th√¥ng tin
- Ch√∫ √Ω ng√†y hi·ªán t·∫°i khi tr·∫£ l·ªùi v·ªÅ "h√¥m nay", "ng√†y mai", etc."""


class OllamaGenerator:
    """
    Ollama LLM Generator for response generation
    Uses qwen2.5:7b model running locally
    """
    
    def __init__(self):
        self.base_url = OLLAMA_BASE_URL
        self.model = OLLAMA_MODEL
        self.client = None
        
    async def _get_client(self) -> httpx.AsyncClient:
        """Get or create async HTTP client"""
        if self.client is None:
            self.client = httpx.AsyncClient(timeout=LLM_TIMEOUT)
        return self.client
    
    async def check_health(self) -> bool:
        """
        Check if Ollama is running and model is available
        
        Returns:
            True if Ollama is healthy
        """
        try:
            client = await self._get_client()
            response = await client.get(f"{self.base_url}/api/tags")
            
            if response.status_code == 200:
                data = response.json()
                models = [m.get('name', '') for m in data.get('models', [])]
                
                # Check if our model is available
                model_available = any(self.model in m for m in models)
                
                if model_available:
                    logger.info(f"‚úÖ Ollama is running with model {self.model}")
                else:
                    logger.warning(f"‚ö†Ô∏è Model {self.model} not found. Available: {models}")
                    logger.warning(f"Run: ollama pull {self.model}")
                
                return model_available
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Ollama health check failed: {e}")
            return False
    
    async def generate(
        self,
        query: str,
        context_docs: List[Dict],
        chat_history: List[Dict] = None,
        temperature: float = None,
        max_tokens: int = None,
        extra_context: str = None
    ) -> str:
        """
        Generate response using RAG context
        
        Args:
            query: User's question
            context_docs: List of retrieved documents with 'content' and 'metadata'
            chat_history: Optional conversation history
            temperature: Override default temperature
            max_tokens: Override default max tokens
            extra_context: Extra context like current date
            
        Returns:
            Generated response text
        """
        client = await self._get_client()
        
        # Build context string from retrieved documents
        context_parts = []
        for i, doc in enumerate(context_docs, 1):
            content = doc.get('content', '')
            metadata = doc.get('metadata', {})
            source_type = metadata.get('source_type', 'unknown')
            score = doc.get('score', 0)
            
            context_parts.append(f"[{i}] (Ngu·ªìn: {source_type}, ƒê·ªô li√™n quan: {score:.2f})\n{content}")
        
        context_str = "\n\n---\n\n".join(context_parts) if context_parts else "Kh√¥ng c√≥ th√¥ng tin li√™n quan."
        
        # Build messages array
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT}
        ]
        
        # Add chat history (last 4 messages for context)
        if chat_history:
            for msg in chat_history[-4:]:
                role = msg.get('role', 'user')
                # Map 'bot' to 'assistant' for Ollama
                if role == 'bot':
                    role = 'assistant'
                messages.append({
                    "role": role,
                    "content": msg.get('content', '')
                })
        
        # Build user prompt with context
        date_context = f"\n\nüìÖ NG√ÄY HI·ªÜN T·∫†I: {extra_context}\n" if extra_context else ""
        
        user_prompt = f"""CONTEXT (Th√¥ng tin li√™n quan):
{context_str}
{date_context}
---

C√ÇU H·ªéI C·ª¶A NG∆Ø·ªúI D√ôNG: {query}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin trong CONTEXT ·ªü tr√™n. N·∫øu kh√¥ng c√≥ th√¥ng tin li√™n quan, h√£y n√≥i r√µ."""
        
        messages.append({"role": "user", "content": user_prompt})
        
        # Call Ollama API
        try:
            response = await client.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": self.model,
                    "messages": messages,
                    "stream": False,
                    "keep_alive": LLM_KEEP_ALIVE,  # Gi·ªØ model trong memory
                    "options": {
                        "temperature": temperature or LLM_TEMPERATURE,
                        "num_predict": max_tokens or LLM_MAX_TOKENS,
                        "num_ctx": 4096  # Gi·∫£m context window ƒë·ªÉ nhanh h∆°n
                    }
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                answer = result.get("message", {}).get("content", "")
                
                if answer:
                    logger.info(f"‚úÖ Generated response ({len(answer)} chars)")
                    return answer.strip()
                else:
                    logger.warning("‚ö†Ô∏è Empty response from Ollama")
                    return "Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i."
            else:
                logger.error(f"‚ùå Ollama API error: {response.status_code} - {response.text}")
                return "Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i sau."
            
        except httpx.TimeoutException:
            logger.error("‚ùå Ollama request timed out")
            return "Xin l·ªói, y√™u c·∫ßu m·∫•t qu√° nhi·ªÅu th·ªùi gian. Vui l√≤ng th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi ng·∫Øn h∆°n."
        except Exception as e:
            logger.error(f"‚ùå Ollama error: {e}")
            return "Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i sau."
    
    async def generate_simple(self, prompt: str) -> str:
        """
        Simple generation without RAG context
        Useful for testing or simple responses
        
        Args:
            prompt: Direct prompt to send
            
        Returns:
            Generated response
        """
        client = await self._get_client()
        
        try:
            response = await client.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": LLM_TEMPERATURE,
                        "num_predict": LLM_MAX_TOKENS
                    }
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "").strip()
            else:
                return f"Error: {response.status_code}"
                
        except Exception as e:
            logger.error(f"‚ùå Simple generation error: {e}")
            return f"Error: {str(e)}"
    
    async def close(self):
        """Close HTTP client"""
        if self.client:
            await self.client.aclose()
            self.client = None
            logger.info("üîå Ollama client closed")


# Singleton instance
llm_generator = OllamaGenerator()


# Test function
async def test_llm_generator():
    """Test LLM generator functionality"""
    print("Testing Ollama LLM Generator...")
    
    generator = OllamaGenerator()
    
    # Check health
    is_healthy = await generator.check_health()
    print(f"Ollama health: {is_healthy}")
    
    if not is_healthy:
        print("‚ùå Ollama is not running or model not available")
        print(f"Please start Ollama and run: ollama pull {OLLAMA_MODEL}")
        return
    
    # Test simple generation
    print("\n--- Simple Generation Test ---")
    response = await generator.generate_simple("Xin ch√†o, b·∫°n l√† ai?")
    print(f"Response: {response[:200]}...")
    
    # Test RAG generation
    print("\n--- RAG Generation Test ---")
    context_docs = [
        {
            "content": "L·ªãch c√¥ng t√°c ng√†y 22/01/2026 (Th·ª© 5)\nTh·ªùi gian: 08:00 - 11:00\nN·ªôi dung: H·ªçp Ban Gi√°m hi·ªáu\nƒê·ªãa ƒëi·ªÉm: Ph√≤ng h·ªçp A1\nCh·ªß tr√¨: Hi·ªáu tr∆∞·ªüng",
            "metadata": {"source_type": "schedule"},
            "score": 0.85
        }
    ]
    
    response = await generator.generate(
        query="H√¥m nay c√≥ l·ªãch g√¨?",
        context_docs=context_docs
    )
    print(f"RAG Response: {response}")
    
    await generator.close()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(test_llm_generator())
